# mosschair.github.io

According to the WHO, there are 65 million wheelchair users in the world, each with unique functional needs. Current products are often tailored to specific disabilities, making it challenging for companies to address the diverse demands within this niche market efficiently. Additionally, many smart wheelchairs are extremely expensive and difficult to understand and even operate. The purpose of the MOSS Chair is to create an open-source platform for smart wheelchairs. This platform will allow developers to implement modular designs, offering a continuously expanding range of features to cater to the varying needs of wheelchair users.

Our approach solves the user's problem by offering an open-source platform with modular designs. The wheelchair will initially feature assistive navigational functions, motorization for autonomous capabilities, computer vision algorithms like Inception-V2 for reliable and fast object detection, and compatibility with a programmable assistive robotic arm. This modular design allows developers to tailor features according to specific user needs, providing a customizable and adaptive solution. Additionally, our project involves community collaboration, which encourages a community of developers to contribute and innovate, expanding the range of available features over time. 

Each component chosen to solve this problem has its purpose. The decision to prioritize modularity and open-source principles is crucial in effectively addressing the various aspects of the customer's problem. Modularity ensures flexibility, allowing the wheelchair to adapt to varying user needs, while open-source principles encourage collaboration and innovation within the community. The chosen conceptual approach stems from the recognition that a rigid, closed-system approach would fall short in addressing the dynamic and diverse landscape of mobility challenges. By adopting a modular, open-source framework, we lay the groundwork for a solution that not only meets present needs but also invites ongoing collaboration, adaptation, and innovation to cater to emerging user requirements.

Our design implements the NVIDIA Jetson Nano as the control hub which handles signal processing for multiple different processes in the design. It utilizes PWM signals to control the Arduino Nano that communicates with the wheel’s motor encoders in order to move the wheelchair in the desired direction and speed. Additionally, the Jetson Nano has computer navigation, with images coming from the connected Raspberry Pi NoIR camera and LiDar sensor, which can be used in the future to move the wheelchair or arm autonomously. 
The Jetson Inference library that we used contained a pre-trained Inception-V2 model trained on Microsoft’s COCO dataset, which features multiple instances of common objects. We imported and trained this model on additional datasets and used it to detect objects in real-time from the Raspberry Pi SC0024 IMX219 NoIR Camera connected to the Jetson Nano. 

All in all, with the combination of sensors, robust motors and the powerful Jetson Nano, the MOSS Chair serves as a sustainable and accessible replacement for the traditional smart wheelchair. 


## Our Modules Thus Far
1. Motorization: we have three different motorization control modes, including key control (from a laptop), voice control, and a joystick.
2. Object Detection: through the Jetson Nano and connected camera, objects can be detected by the wheelchair.
3. Robotic Arm: The current state of the project is having a functional hand that can articulate all fingers and rotate the wrist. A big problem is going past that, ie past an elbow, to the shoulder. The reason for this is that the torque needed becomes pretty large and requires the construction of a gearbox to allow for a motor to put out these torques. But then you need a drive shaft running through the arm and a system of gears at the base of the elbow and shoulder that are part of the physical arm where the turn will happen. It becomes very complicated and it is not likely something that can be 3D printed unless perhaps you can find a printer with a thick extrusion and use a strong material. Anyway, it would be easier to use the pulley system that the current arm uses, the parts for which can be found below in the hardware section, and the guides here:

https://inmoov.fr/hand-and-forarm/
https://inmoov.fr/lining-and-tighting-the-tendons/
https://inmoov.fr/bicep/

We would, however, not recommend using this same design. This is because the project aims to be open source and accessible to people, and while this is possible with the current model, it is overly artistic and somewhat impractical. There are too many parts that are honestly too small and it requires a lot of post machining that makes the project less attainable for most people. A big change I would make is changing the entire end effector (the hand) into one that has just two fingers, or 3 at most, which are aligned as a claw or pincer. I would also suggest turning the body of the arm into either a rectangular prism or cylinder which has a slight taper towards the end, as it is much easier to print and doesn’t have a ‘weaker’ side in terms of load bearing.

We would recommend the use of the wrist as it currently exists however, as it functions extremely well and if not limited by the degrees available to the motor, could rotate a couple times over. The pulley system, as mentioned before, also works well and is only limited by the strength of your ‘tendon,’ but it is easy to just buy something stronger if you need. It can easily be incorporated into a claw-like end effector.

As we said before, the elbow and shoulder joints are difficult to manage as they require a lot of torque which can become expensive, and as we stated earlier, the goal of this project is accessibility. As you will find when you undoubtedly Google “wheelchair robotic arm,” or the such, the arms that attach to wheelchairs and function as an appendage with moving joints and such costs between $35,000 to $50,000. A lot of this is the systems enabling the arm to move in the first place and enabling it to hold its own weight as well as that of an object to pick up. If you are determined or ambitious enough to try that anyway, I would suggest looking at the “VESA Mount” for a good starting point on having an arm that can be moved and be able to maintain that position, even with an added weight. You might need to scale up the mount, but for a small prototype or simply to understand the mechanisms behind how it works, we think it is a very good idea, and it costs a much cheaper $200ish, depending on model. You then would just need to modify it to be able to remotely force movement.

Another avenue we would advise on is forgoing the need for a normal “elbow” or “shoulder” entirely and strapping the end effector to a platform of some sort which can itself be lowered or raised. This adjustable platform would then be mounted to a base that itself could move about remotely. This has the benefit of not requiring any heavy and expensive high-torque motors and is very structurally stable. Additionally, an arm that can move independently of the wheelchair solves certain issues such as not having enough reach to grab something, and doing things like holding a door open, while the wheelchair user goes through it before then going through it and letting it close. This is difficult to accomplish when the arm is attached to the wheelchair, and you must hold and open the door, while maneuvering the wheelchair the whole time, and then keep it open while the wheelchair is going through it. A good example of such an arm would be the “Stretch 3 Robotic Arm,” by Hello Robot. While also expensive at some $25,000, it is much cheaper than the ‘full’ arms, while also being a stand-alone attachment. On top of that, the website also has the code for the arm free and open source, the link to the website and their GitHub is below. They put a camera on theirs too which is a nice addition that’s easily doable, and can likely have the object detection from other parts of this project easily integrated into it.

https://hello-robot.com/stretch-3-product
https://github.com/hello-robot

There is also a new version of the inmoov hand that functions using springs and is better, but I have not looked into it. I do think it is worth checking out though as you may end up deciding that your arm and end effector will be a mix of all the various models you find, the link is below.

https://inmoov.fr/inmoov-hand/



## Future Recommendations
1. Obstacle detection and avoidance: We have looked into ROS and LiDAR sensing for the wheelchair so that the wheelchair is able to drive autonomously. For future iterations of this project, we recommend looking into autonomous driving and control through LiDAR and ROS. This would be especially useful for indoor navigation as small objects would be avoided easily.
2. Eye movement/brain sensing: We recommend developing another control module for wheelchair motorization for users who have limited hand movement. We recommend looking into eye sensors and integrating these sensors with the motorization of the wheelchair so that users can move the wheelchair with their eyes. Another mode that could be looked into is sensing brain waves through EEG nodes or other sensors. By directly reading into the user's brain waves, the wheelchair would be able to be moved with minimal effort from the user. This would be more inclusive for users with hand mobility issues who may not be able to use our existing control modes like the keys or joystick.
3. Change Encoder Mounting: As of now, the rotary encoders are mounted directly on to the wheels. However this is not the best approach for multiple reasons, first and foremost it does not fit into our modular framework as well as mounting it directly to the motors would. It is also less precise, and more invasive onto the chair itself as screw are needed to fasten the encoder to the wheel. 
4. Integrating object detection with the robotic arm: by integrating object detection with the arm, the arm will be able to move as per the user's desires. We envision the arm being able to push elevator buttons, open and close doors, and pick up small items. So far, we have the object detection working independently of the arm. Additionally, we have the arm 3D-printed out and equipped with high-torque servo motors. These two modules should be integrated together to create a high-level and more user-friendly module capable of completing essential tasks that may be impossible for people with arm mobility issues.

## Things to Look Out For
1. Loose wires! Make sure that all wires are always fully connected. Even one loose wire can lead to erratic motor movement.
2. Battery charging: As of right now, we are using 2 24 V AGM batteries connected in parallel. These batteries get drained after extraneous work and need to be charged regularly.
3. Encoders: Make sure encoders are always connected to the spokes well to make sure that the motors can actually move the wheelchair.
4. Arm strings: Make sure the strings connected to the fingers of the arm are taut at all points. If the strings get loose, the arm will not be able to move as smoothly or strongly as envisioned.
